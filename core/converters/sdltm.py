# core/converters/sdltm.py - –ü–û–õ–ù–ê–Ø –í–ï–†–°–ò–Ø —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º–∏ –ª–æ–≥–∞–º–∏

import sqlite3
import xml.etree.ElementTree as ET
from pathlib import Path
from typing import Iterator, Tuple, Dict, Set, Optional, List
import logging
import time
from datetime import datetime

from ..base import (
    FileConverter, ConversionResult, ConversionOptions, StreamingConverter,
    ConversionError, ValidationError, ConversionStatus
)
from ..formats.tmx_format import TmxWriter
from ..formats.xlsx_format import XlsxWriter

logger = logging.getLogger(__name__)


class SdltmConverter(StreamingConverter):
    """
    –ü–æ–ª–Ω—ã–π –∫–æ–Ω–≤–µ—Ä—Ç–µ—Ä –¥–ª—è SDLTM —Ñ–∞–π–ª–æ–≤ —Å –ø–æ—Ç–æ–∫–æ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º–∏ –ª–æ–≥–∞–º–∏.

    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç:
    - –ü–æ—Ç–æ–∫–æ–≤—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤
    - –ê–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤
    - –≠–∫—Å–ø–æ—Ä—Ç –≤ TMX, XLSX, JSON
    - –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—é —Å–µ–≥–º–µ–Ω—Ç–æ–≤
    - –î–µ—Ç–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    - –û–±—Ä–∞–±–æ—Ç–∫—É –æ—à–∏–±–æ–∫ –∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ
    - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –ª–æ–≥–æ–≤
    - –û—á–∏—Å—Ç–∫—É SQLite –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
    """

    def __init__(self):
        super().__init__()
        self.supported_exports = {'tmx', 'xlsx', 'json'}
        self.language_cache = {}  # –ö—ç—à –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —è–∑—ã–∫–æ–≤

    def can_handle(self, filepath: Path) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –º–æ–∂–µ—Ç –ª–∏ –∫–æ–Ω–≤–µ—Ä—Ç–µ—Ä –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ñ–∞–π–ª"""
        return filepath.suffix.lower() == '.sdltm'

    def validate(self, filepath: Path) -> bool:
        """
        –í–∞–ª–∏–¥–∏—Ä—É–µ—Ç SDLTM —Ñ–∞–π–ª –Ω–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—ã

        Args:
            filepath: –ü—É—Ç—å –∫ SDLTM —Ñ–∞–π–ª—É

        Returns:
            True –µ—Å–ª–∏ —Ñ–∞–π–ª –≤–∞–ª–∏–¥–µ–Ω

        Raises:
            ValidationError: –ü—Ä–∏ –æ—à–∏–±–∫–∞—Ö –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        """
        if not filepath.exists():
            raise ValidationError(f"File not found: {filepath}")

        if not filepath.is_file():
            raise ValidationError(f"Not a file: {filepath}")

        try:
            with sqlite3.connect(str(filepath)) as conn:
                cursor = conn.cursor()

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ SQLite —Ñ–∞–π–ª
                cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
                tables = {row[0] for row in cursor.fetchall()}

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã
                if 'translation_units' not in tables:
                    raise ValidationError(f"Missing translation_units table in {filepath}")

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ç–∞–±–ª–∏—Ü—ã
                cursor.execute("PRAGMA table_info(translation_units)")
                columns = {row[1] for row in cursor.fetchall()}

                required_columns = {'source_segment', 'target_segment'}
                missing_columns = required_columns - columns

                if missing_columns:
                    raise ValidationError(f"Missing columns {missing_columns} in {filepath}")

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ
                cursor.execute("SELECT COUNT(*) FROM translation_units")
                count = cursor.fetchone()[0]

                if count == 0:
                    logger.warning(f"Empty SDLTM file: {filepath}")
                    return True  # –ü—É—Å—Ç–æ–π —Ñ–∞–π–ª —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏ –≤–∞–ª–∏–¥–µ–Ω

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ –∑–∞–ø–∏—Å–µ–π –Ω–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å XML
                cursor.execute("SELECT source_segment, target_segment FROM translation_units LIMIT 10")

                valid_segments = 0
                for src_xml, tgt_xml in cursor.fetchall():
                    try:
                        self._parse_segment_xml(src_xml)
                        self._parse_segment_xml(tgt_xml)
                        valid_segments += 1
                    except Exception as e:
                        logger.debug(f"Invalid segment XML: {e}")
                        continue

                if valid_segments == 0:
                    raise ValidationError(f"No valid segments found in {filepath}")

                logger.info(f"SDLTM validation successful: {filepath} ({count} segments)")
                return True

        except sqlite3.Error as e:
            raise ValidationError(f"Database error in {filepath}: {e}")
        except Exception as e:
            raise ValidationError(f"Unexpected error validating {filepath}: {e}")

    def get_progress_steps(self, filepath: Path) -> int:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞

        Args:
            filepath: –ü—É—Ç—å –∫ SDLTM —Ñ–∞–π–ª—É

        Returns:
            –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –≤ —Ñ–∞–π–ª–µ
        """
        try:
            with sqlite3.connect(str(filepath)) as conn:
                cursor = conn.cursor()
                cursor.execute("SELECT COUNT(*) FROM translation_units")
                return cursor.fetchone()[0]
        except sqlite3.Error as e:
            logger.error(f"Error getting progress steps for {filepath}: {e}")
            return 0

    def convert(self, filepath: Path, options: ConversionOptions) -> ConversionResult:
        """
        –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç SDLTM —Ñ–∞–π–ª –≤ —É–∫–∞–∑–∞–Ω–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º —Å–æ–∑–¥–∞–Ω–∏–µ–º –ª–æ–≥–æ–≤

        Args:
            filepath: –ü—É—Ç—å –∫ SDLTM —Ñ–∞–π–ª—É
            options: –û–ø—Ü–∏–∏ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏

        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏
        """
        start_time = time.time()

        # –ù–û–í–û–ï: –°–æ–±–∏—Ä–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –ª–æ–≥–æ–≤
        detailed_stats = {
            "skipped_details": {
                "empty": [],
                "tags_only": [],
                "duplicates": [],
                "errors": []
            }
        }

        try:
            # –í–∞–ª–∏–¥–∞—Ü–∏—è —Ñ–∞–π–ª–∞
            self._update_progress(0, "Validating file...", options)
            if not self.validate(filepath):
                return ConversionResult(
                    success=False,
                    output_files=[],
                    stats={"error": "Validation failed"},
                    errors=["File validation failed"],
                    status=ConversionStatus.FAILED
                )

            # –ü–æ–ª—É—á–∞–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–µ–≥–º–µ–Ω—Ç–æ–≤
            total_segments = self.get_progress_steps(filepath)
            if total_segments == 0:
                return ConversionResult(
                    success=False,
                    output_files=[],
                    stats={"total": 0},
                    errors=["No segments found"],
                    status=ConversionStatus.FAILED
                )

            self._update_progress(5, f"Found {total_segments:,} segments", options)

            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            stats = {
                "total_in_sdltm": total_segments,
                "processed": 0,
                "exported": 0,
                "skipped_empty": 0,
                "skipped_tags_only": 0,
                "skipped_duplicates": 0,
                "skipped_errors": 0,
                "languages_detected": {},
                "conversion_time": 0,
                "memory_used_mb": 0
            }

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —è–∑—ã–∫–∏
            self._update_progress(10, "Detecting languages...", options)
            detected_languages = self._detect_languages(filepath)
            stats["languages_detected"] = detected_languages

            src_lang = self._resolve_language(options.source_lang, detected_languages.get('source', 'en-US'))
            tgt_lang = self._resolve_language(options.target_lang, detected_languages.get('target', 'ru-RU'))

            logger.info(f"Using languages: {src_lang} ‚Üí {tgt_lang}")

            # –ü–æ—Ç–æ–∫–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å–µ–≥–º–µ–Ω—Ç–æ–≤
            self._update_progress(15, "Processing segments...", options)

            segments = []
            seen_pairs = set()

            # –û–ë–ù–û–í–õ–ï–ù–û: –°–æ–±–∏—Ä–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö —Å–µ–≥–º–µ–Ω—Ç–∞—Ö
            for segment_data in self.convert_streaming_detailed(filepath, options):
                if self._should_stop(options):
                    logger.info("Conversion stopped by user")
                    return ConversionResult(
                        success=False,
                        output_files=[],
                        stats=stats,
                        errors=["Conversion cancelled by user"],
                        status=ConversionStatus.CANCELLED
                    )

                src_text, tgt_text, seg_src_lang, seg_tgt_lang, skip_reason = segment_data
                stats["processed"] += 1

                # –ù–û–í–û–ï: –°–æ–±–∏—Ä–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –¥–ª—è –ª–æ–≥–æ–≤
                if skip_reason:
                    example = (src_text[:100] + "..." if len(src_text) > 100 else src_text,
                               tgt_text[:100] + "..." if len(tgt_text) > 100 else tgt_text)

                    if skip_reason == "empty":
                        stats["skipped_empty"] += 1
                        if len(detailed_stats["skipped_details"]["empty"]) < 10:  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–µ—Ä–≤—ã–µ 10 –ø—Ä–∏–º–µ—Ä–æ–≤
                            detailed_stats["skipped_details"]["empty"].append(example)
                    elif skip_reason == "tags_only":
                        stats["skipped_tags_only"] += 1
                        if len(detailed_stats["skipped_details"]["tags_only"]) < 10:
                            detailed_stats["skipped_details"]["tags_only"].append(example)
                    elif skip_reason == "error":
                        stats["skipped_errors"] += 1
                        if len(detailed_stats["skipped_details"]["errors"]) < 10:
                            detailed_stats["skipped_details"]["errors"].append(example)
                    continue

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã
                pair_key = (src_text.strip(), tgt_text.strip())
                if pair_key in seen_pairs:
                    stats["skipped_duplicates"] += 1
                    # –ù–û–í–û–ï: –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–∏–º–µ—Ä—ã –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
                    if len(detailed_stats["skipped_details"]["duplicates"]) < 10:
                        example = (src_text[:100] + "..." if len(src_text) > 100 else src_text,
                                   tgt_text[:100] + "..." if len(tgt_text) > 100 else tgt_text)
                        detailed_stats["skipped_details"]["duplicates"].append(example)
                    continue

                seen_pairs.add(pair_key)

                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —è–∑—ã–∫–∏ –∏–ª–∏ –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–µ
                final_src_lang = seg_src_lang if seg_src_lang != "unknown" else src_lang
                final_tgt_lang = seg_tgt_lang if seg_tgt_lang != "unknown" else tgt_lang

                segments.append((src_text, tgt_text, final_src_lang, final_tgt_lang))
                stats["exported"] += 1

            if not segments:
                return ConversionResult(
                    success=False,
                    output_files=[],
                    stats=stats,
                    errors=["No valid segments found after processing"],
                    status=ConversionStatus.FAILED
                )

            # –≠–∫—Å–ø–æ—Ä—Ç –≤ —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã
            self._update_progress(80, "Writing output files...", options)
            output_files = []

            try:
                # TMX —ç–∫—Å–ø–æ—Ä—Ç
                if options.export_tmx:
                    tmx_path = filepath.with_suffix('.tmx')
                    TmxWriter.write(tmx_path, segments, src_lang, tgt_lang)
                    output_files.append(tmx_path)
                    logger.info(f"TMX created: {tmx_path}")

                # XLSX —ç–∫—Å–ø–æ—Ä—Ç
                if options.export_xlsx:
                    xlsx_path = filepath.with_suffix('.xlsx')
                    XlsxWriter.write(xlsx_path, segments, src_lang, tgt_lang)
                    output_files.append(xlsx_path)
                    logger.info(f"XLSX created: {xlsx_path}")

                # JSON —ç–∫—Å–ø–æ—Ä—Ç
                if getattr(options, 'export_json', False):
                    json_path = filepath.with_suffix('.json')
                    self._write_json(json_path, segments, src_lang, tgt_lang)
                    output_files.append(json_path)
                    logger.info(f"JSON created: {json_path}")

                # –ù–û–í–û–ï: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –ª–æ–≥–∞
                log_path = filepath.with_suffix('.conversion-log.txt')
                self._write_conversion_log(log_path, filepath, stats, detailed_stats, src_lang, tgt_lang, output_files)
                logger.info(f"Conversion log created: {log_path}")

            except Exception as e:
                logger.error(f"Error writing output files: {e}")
                return ConversionResult(
                    success=False,
                    output_files=output_files,  # –ß–∞—Å—Ç–∏—á–Ω–æ —Å–æ–∑–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
                    stats=stats,
                    errors=[f"Export error: {e}"],
                    status=ConversionStatus.FAILED
                )

            # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            stats["conversion_time"] = time.time() - start_time
            stats["memory_used_mb"] = self._get_memory_usage()

            self._update_progress(100, f"Completed! Exported {stats['exported']:,} segments", options)

            # –ü–æ–¥—Ä–æ–±–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤ –ª–æ–≥
            self._log_conversion_summary(filepath, stats, src_lang, tgt_lang)

            return ConversionResult(
                success=True,
                output_files=output_files,
                stats=stats,
                status=ConversionStatus.COMPLETED
            )

        except Exception as e:
            logger.exception(f"Critical error converting {filepath}")
            return ConversionResult(
                success=False,
                output_files=[],
                stats={"error": str(e), "conversion_time": time.time() - start_time},
                errors=[str(e)],
                status=ConversionStatus.FAILED
            )

    def convert_streaming(self, filepath: Path, options: ConversionOptions) -> Iterator[Tuple[str, str, str, str]]:
        """
        –ü–æ—Ç–æ–∫–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ SDLTM —Ñ–∞–π–ª–∞ —Å –±–∞—Ç—á–∞–º–∏ (–æ–±—ã—á–Ω–∞—è –≤–µ—Ä—Å–∏—è)

        Args:
            filepath: –ü—É—Ç—å –∫ SDLTM —Ñ–∞–π–ª—É
            options: –û–ø—Ü–∏–∏ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏

        Yields:
            Tuple[src_text, tgt_text, src_lang, tgt_lang]
        """
        for segment_data in self.convert_streaming_detailed(filepath, options):
            src_text, tgt_text, src_lang, tgt_lang, skip_reason = segment_data
            if skip_reason is None:  # –¢–æ–ª—å–∫–æ –≤–∞–ª–∏–¥–Ω—ã–µ —Å–µ–≥–º–µ–Ω—Ç—ã
                yield (src_text, tgt_text, src_lang, tgt_lang)

    def convert_streaming_detailed(self, filepath: Path, options: ConversionOptions):
        """
        –ò–°–ü–†–ê–í–õ–ï–ù–û: –ü–æ—Ç–æ–∫–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–µ–∑ WAL —Ä–µ–∂–∏–º–∞ –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –æ—à–∏–±–æ–∫

        Args:
            filepath: –ü—É—Ç—å –∫ SDLTM —Ñ–∞–π–ª—É
            options: –û–ø—Ü–∏–∏ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏

        Yields:
            Tuple[src_text, tgt_text, src_lang, tgt_lang, skip_reason]
            skip_reason: None –µ—Å–ª–∏ —Å–µ–≥–º–µ–Ω—Ç –≤–∞–ª–∏–¥–µ–Ω, –∏–Ω–∞—á–µ –ø—Ä–∏—á–∏–Ω–∞ –ø—Ä–æ–ø—É—Å–∫–∞
        """
        conn = None
        cursor = None

        try:
            # –ò–°–ü–†–ê–í–õ–ï–ù–û: –°–æ–∑–¥–∞–µ–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –±–µ–∑ WAL —Ä–µ–∂–∏–º–∞
            conn = sqlite3.connect(str(filepath))

            # –ë–µ–∑–æ–ø–∞—Å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –±–µ–∑ WAL
            conn.execute("PRAGMA synchronous=NORMAL")
            conn.execute("PRAGMA cache_size=10000")
            conn.execute("PRAGMA temp_store=MEMORY")
            conn.execute("PRAGMA read_uncommitted=1")  # –î–ª—è —á—Ç–µ–Ω–∏—è

            cursor = conn.cursor()

            # –ü–æ–ª—É—á–∞–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
            cursor.execute("SELECT COUNT(*) FROM translation_units")
            total = cursor.fetchone()[0]

            logger.info(f"Starting streaming conversion of {total:,} segments")

            processed = 0
            batch_size = getattr(options, 'batch_size', 1000)
            offset = 0

            while True:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –æ—Å—Ç–∞–Ω–æ–≤–∫—É
                if self._should_stop(options):
                    logger.info("SDLTM streaming conversion stopped by user")
                    break

                # –ß–∏—Ç–∞–µ–º –±–∞—Ç—á —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
                try:
                    cursor.execute(
                        "SELECT source_segment, target_segment FROM translation_units LIMIT ? OFFSET ?",
                        (batch_size, offset)
                    )
                    batch = cursor.fetchall()

                    if not batch:
                        logger.info(f"Finished reading all segments at offset {offset}")
                        break

                except sqlite3.Error as e:
                    logger.error(f"Database error at offset {offset}: {e}")
                    # –ü—Ä–æ–±—É–µ–º –ø–µ—Ä–µ–ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è
                    try:
                        if cursor:
                            cursor.close()
                        if conn:
                            conn.close()

                        conn = sqlite3.connect(str(filepath))
                        cursor = conn.cursor()
                        continue
                    except Exception:
                        break

                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±–∞—Ç—á
                batch_processed = 0
                for src_xml, tgt_xml in batch:
                    processed += 1
                    batch_processed += 1

                    try:
                        # –ü–∞—Ä—Å–∏–º —Å–µ–≥–º–µ–Ω—Ç—ã
                        src_text, src_lang = self._parse_segment_xml(src_xml)
                        tgt_text, tgt_lang = self._parse_segment_xml(tgt_xml)

                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –ø—É—Å—Ç—ã–µ —Å–µ–≥–º–µ–Ω—Ç—ã
                        if not src_text.strip() or not tgt_text.strip():
                            yield (src_text, tgt_text, src_lang, tgt_lang, "empty")
                            continue

                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–µ–≥–º–µ–Ω—Ç—ã —Ç–æ–ª—å–∫–æ —Å —Ç–µ–≥–∞–º–∏
                        if self._is_tags_only(src_xml) or self._is_tags_only(tgt_xml):
                            yield (src_text, tgt_text, src_lang, tgt_lang, "tags_only")
                            continue

                        # –í–∞–ª–∏–¥–Ω—ã–π —Å–µ–≥–º–µ–Ω—Ç
                        yield (src_text, tgt_text, src_lang, tgt_lang, None)

                    except Exception as e:
                        logger.debug(f"Error parsing segment {processed}: {e}")
                        yield ("", "", "unknown", "unknown", "error")
                        continue

                # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
                if total > 0:
                    progress = 15 + int((processed / total) * 60)  # 15-75% –¥–∏–∞–ø–∞–∑–æ–Ω
                    self._update_progress(progress, f"Processed {processed:,}/{total:,} segments", options)

                # –õ–æ–≥–∏—Ä—É–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –±–∞—Ç—á–∞
                logger.debug(f"Processed batch: {batch_processed} segments, total: {processed}/{total}")

                offset += batch_size

            logger.info(f"Streaming conversion completed: {processed} segments processed")

        except Exception as e:
            logger.error(f"Critical error in streaming conversion: {e}")
            raise ConversionError(f"Streaming conversion failed: {e}", filepath)

        finally:
            # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ë–µ–∑–æ–ø–∞—Å–Ω–æ –∑–∞–∫—Ä—ã–≤–∞–µ–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ
            try:
                if cursor:
                    cursor.close()
                    logger.debug("Cursor closed")

                if conn:
                    conn.close()
                    logger.debug("Database connection closed")

                # –û—á–∏—â–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã SQLite –ø–æ—Å–ª–µ –∑–∞–∫—Ä—ã—Ç–∏—è —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
                self._cleanup_sqlite_temp_files(filepath)

            except Exception as e:
                logger.debug(f"Error closing database connection: {e}")

    def _cleanup_sqlite_temp_files(self, filepath: Path):
        """–ù–û–í–û–ï: –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ SQLite –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤"""
        try:
            import time
            # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤
            time.sleep(0.1)

            cleaned_files = []
            for suffix in ['-wal', '-shm', '-journal']:
                temp_file = Path(str(filepath) + suffix)
                if temp_file.exists():
                    try:
                        temp_file.unlink()
                        cleaned_files.append(temp_file.name)
                        logger.debug(f"Cleaned up SQLite temp file: {temp_file}")
                    except Exception as e:
                        logger.debug(f"Could not clean temp file {temp_file}: {e}")

            if cleaned_files:
                logger.info(f"Cleaned up SQLite temporary files: {', '.join(cleaned_files)}")

        except Exception as e:
            logger.debug(f"Error during temp files cleanup: {e}")

    def _parse_segment_xml(self, xml_segment: str) -> Tuple[str, str]:
        """
        –ü–∞—Ä—Å–∏—Ç XML —Å–µ–≥–º–µ–Ω—Ç SDLTM —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫

        Args:
            xml_segment: XML —Å—Ç—Ä–æ–∫–∞ —Å–µ–≥–º–µ–Ω—Ç–∞

        Returns:
            Tuple[text, language_code]
        """
        if not xml_segment or not xml_segment.strip():
            return "", "unknown"

        try:
            # –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            if xml_segment in self.language_cache:
                return self.language_cache[xml_segment]

            root = ET.fromstring(xml_segment)

            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —Å–ø–æ—Å–æ–±–∞–º–∏
            text = ""

            # –û—Å–Ω–æ–≤–Ω–æ–π —Å–ø–æ—Å–æ–± - Text/Value
            text_elem = root.find(".//Text/Value")
            if text_elem is not None and text_elem.text:
                text = text_elem.text
            else:
                # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ —Å–ø–æ—Å–æ–±—ã
                for xpath in [".//Value", ".//Text", ".//Content"]:
                    elem = root.find(xpath)
                    if elem is not None and elem.text:
                        text = elem.text
                        break

            # –ò–∑–≤–ª–µ–∫–∞–µ–º —è–∑—ã–∫
            lang = "unknown"
            for xpath in [".//CultureName", ".//Culture", ".//Language", ".//Lang"]:
                lang_elem = root.find(xpath)
                if lang_elem is not None and lang_elem.text:
                    lang = self._normalize_language(lang_elem.text)
                    break

            result = (text.strip(), lang)

            # –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            if len(self.language_cache) < 1000:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –∫—ç—à–∞
                self.language_cache[xml_segment] = result

            return result

        except ET.ParseError as e:
            logger.debug(f"XML parsing error: {e}")
            return "", "unknown"
        except Exception as e:
            logger.debug(f"Unexpected error parsing XML: {e}")
            return "", "unknown"

    def _is_tags_only(self, xml_segment: str) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —Å–æ—Å—Ç–æ–∏—Ç –ª–∏ —Å–µ–≥–º–µ–Ω—Ç —Ç–æ–ª—å–∫–æ –∏–∑ —Ç–µ–≥–æ–≤ –±–µ–∑ —Ç–µ–∫—Å—Ç–∞

        Args:
            xml_segment: XML —Å—Ç—Ä–æ–∫–∞ —Å–µ–≥–º–µ–Ω—Ç–∞

        Returns:
            True –µ—Å–ª–∏ —Å–µ–≥–º–µ–Ω—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–æ–ª—å–∫–æ —Ç–µ–≥–∏
        """
        if not xml_segment or not xml_segment.strip():
            return True

        try:
            root = ET.fromstring(xml_segment)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ç–µ–∫—Å—Ç–∞
            text_elem = root.find(".//Text/Value")
            if text_elem is None:
                return True

            text_content = text_elem.text or ""
            text_content = text_content.strip()

            if not text_content:
                return True

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ç–µ–≥–æ–≤
            tags = root.findall(".//Tag")

            # –ï—Å–ª–∏ –µ—Å—Ç—å —Ç–µ–≥–∏, –Ω–æ –Ω–µ—Ç –∑–Ω–∞—á–∏–º–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
            if tags and len(text_content) < 3:
                return True

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ç–µ–∫—Å—Ç –Ω–µ —Å–æ—Å—Ç–æ–∏—Ç —Ç–æ–ª—å–∫–æ –∏–∑ –ø—Ä–æ–±–µ–ª–æ–≤ –∏ —Å–∏–º–≤–æ–ª–æ–≤
            if text_content and not any(c.isalnum() for c in text_content):
                return True

            return False

        except ET.ParseError:
            return True
        except Exception:
            return True

    def _detect_languages(self, filepath: Path) -> Dict[str, str]:
        """
        –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —è–∑—ã–∫–∏ –∏–∑ SDLTM —Ñ–∞–π–ª–∞

        Args:
            filepath: –ü—É—Ç—å –∫ SDLTM —Ñ–∞–π–ª—É

        Returns:
            Dict —Å –∫–ª—é—á–∞–º–∏ 'source' –∏ 'target'
        """
        detected = {"source": "unknown", "target": "unknown"}

        try:
            with sqlite3.connect(str(filepath)) as conn:
                cursor = conn.cursor()

                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–µ—Ä–≤—ã–µ 50 —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —è–∑—ã–∫–æ–≤
                cursor.execute("SELECT source_segment, target_segment FROM translation_units LIMIT 50")

                src_langs = {}
                tgt_langs = {}

                for src_xml, tgt_xml in cursor.fetchall():
                    try:
                        _, src_lang = self._parse_segment_xml(src_xml)
                        _, tgt_lang = self._parse_segment_xml(tgt_xml)

                        if src_lang != "unknown":
                            src_langs[src_lang] = src_langs.get(src_lang, 0) + 1

                        if tgt_lang != "unknown":
                            tgt_langs[tgt_lang] = tgt_langs.get(tgt_lang, 0) + 1

                    except Exception:
                        continue

                # –í—ã–±–∏—Ä–∞–µ–º –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç—ã–µ —è–∑—ã–∫–∏
                if src_langs:
                    detected["source"] = max(src_langs, key=src_langs.get)
                    logger.info(f"Auto-detected source language: {detected['source']}")

                if tgt_langs:
                    detected["target"] = max(tgt_langs, key=tgt_langs.get)
                    logger.info(f"Auto-detected target language: {detected['target']}")

        except Exception as e:
            logger.warning(f"Error detecting languages: {e}")

        return detected

    def _resolve_language(self, option_lang: str, detected_lang: str) -> str:
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —è–∑—ã–∫ –∏–∑ –æ–ø—Ü–∏–π –∏–ª–∏ –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è

        Args:
            option_lang: –Ø–∑—ã–∫ –∏–∑ –æ–ø—Ü–∏–π
            detected_lang: –ê–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π —è–∑—ã–∫

        Returns:
            –§–∏–Ω–∞–ª—å–Ω—ã–π –∫–æ–¥ —è–∑—ã–∫–∞
        """
        if option_lang and option_lang.lower() not in ["auto", "unknown", ""]:
            return self._normalize_language(option_lang)
        return detected_lang

    def _normalize_language(self, lang_code: str) -> str:
        """
        –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —è–∑—ã–∫–æ–≤–æ–π –∫–æ–¥ –∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º—É —Ñ–æ—Ä–º–∞—Ç—É

        Args:
            lang_code: –ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ —è–∑—ã–∫–∞

        Returns:
            –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –∫–æ–¥ —è–∑—ã–∫–∞
        """
        if not lang_code or lang_code.lower() in ["unknown", ""]:
            return "unknown"

        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –∑–∞–º–µ–Ω—ã
        lang_map = {
            "en": "en-US", "english": "en-US",
            "de": "de-DE", "german": "de-DE", "deutsch": "de-DE",
            "fr": "fr-FR", "french": "fr-FR", "fran√ßais": "fr-FR",
            "it": "it-IT", "italian": "it-IT", "italiano": "it-IT",
            "es": "es-ES", "spanish": "es-ES", "espa√±ol": "es-ES",
            "pt": "pt-PT", "portuguese": "pt-PT", "portugu√™s": "pt-PT",
            "ru": "ru-RU", "russian": "ru-RU", "—Ä—É—Å—Å–∫–∏–π": "ru-RU",
            "ja": "ja-JP", "japanese": "ja-JP", "Êó•Êú¨Ë™û": "ja-JP",
            "ko": "ko-KR", "korean": "ko-KR", "ÌïúÍµ≠Ïñ¥": "ko-KR",
            "zh": "zh-CN", "chinese": "zh-CN", "‰∏≠Êñá": "zh-CN",
            "pl": "pl-PL", "polish": "pl-PL", "polski": "pl-PL",
            "tr": "tr-TR", "turkish": "tr-TR", "t√ºrk√ße": "tr-TR",
            "nl": "nl-NL", "dutch": "nl-NL", "nederlands": "nl-NL",
            "sv": "sv-SE", "swedish": "sv-SE", "svenska": "sv-SE",
            "da": "da-DK", "danish": "da-DK", "dansk": "da-DK",
            "no": "no-NO", "norwegian": "no-NO", "norsk": "no-NO",
            "fi": "fi-FI", "finnish": "fi-FI", "suomi": "fi-FI"
        }

        code = lang_code.lower().strip().replace("_", "-")

        # –ï—Å–ª–∏ —É–∂–µ –ø–æ–ª–Ω—ã–π –∫–æ–¥ (–Ω–∞–ø—Ä–∏–º–µ—Ä, en-US)
        if "-" in code and len(code) == 5:
            return code

        # –ò—â–µ–º –≤ –∫–∞—Ä—Ç–µ –∑–∞–º–µ–Ω
        if code in lang_map:
            return lang_map[code]

        # –ï—Å–ª–∏ —ç—Ç–æ –¥–≤—É—Ö–±—É–∫–≤–µ–Ω–Ω—ã–π –∫–æ–¥, –¥–æ–±–∞–≤–ª—è–µ–º —Ä–µ–≥–∏–æ–Ω –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        if len(code) == 2 and code.isalpha():
            return lang_map.get(code, f"{code}-XX")

        # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –ø–æ–¥–æ—à–ª–æ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å
        return lang_code

    def _write_json(self, filepath: Path, segments, src_lang: str, tgt_lang: str):
        """
        –ó–∞–ø–∏—Å—ã–≤–∞–µ—Ç JSON —Ñ–∞–π–ª —Å —Å–µ–≥–º–µ–Ω—Ç–∞–º–∏

        Args:
            filepath: –ü—É—Ç—å –∫ JSON —Ñ–∞–π–ª—É
            segments: –°–ø–∏—Å–æ–∫ —Å–µ–≥–º–µ–Ω—Ç–æ–≤
            src_lang: –ò—Å—Ö–æ–¥–Ω—ã–π —è–∑—ã–∫
            tgt_lang: –¶–µ–ª–µ–≤–æ–π —è–∑—ã–∫
        """
        import json

        data = {
            "metadata": {
                "source_language": src_lang,
                "target_language": tgt_lang,
                "created_at": datetime.now().isoformat(),
                "total_segments": len(segments),
                "created_by": "Converter Pro v2.0"
            },
            "segments": []
        }

        for i, (src_text, tgt_text, seg_src_lang, seg_tgt_lang) in enumerate(segments):
            data["segments"].append({
                "id": i + 1,
                "source": {
                    "text": src_text,
                    "language": seg_src_lang
                },
                "target": {
                    "text": tgt_text,
                    "language": seg_tgt_lang
                }
            })

        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

    def _write_conversion_log(self, log_path: Path, source_file: Path, stats: Dict,
                              detailed_stats: Dict, src_lang: str, tgt_lang: str, output_files: List[Path]):
        """–ù–û–í–û–ï: –°–æ–∑–¥–∞–µ—Ç –∫—Ä–∞—Å–∏–≤—ã–π –∏ —á–∏—Ç–∞–µ–º—ã–π –ª–æ–≥ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏"""
        try:
            with open(log_path, 'w', encoding='utf-8') as f:
                # –ó–∞–≥–æ–ª–æ–≤–æ–∫
                f.write("=" * 80 + "\n")
                f.write("üîÑ CONVERSION LOG - CONVERTER PRO v2.0\n")
                f.write("=" * 80 + "\n")
                f.write(f"üìÖ –î–∞—Ç–∞: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"üìÅ –ò—Å—Ö–æ–¥–Ω—ã–π —Ñ–∞–π–ª: {source_file.name}\n")
                f.write(f"üìÇ –ü—É—Ç—å: {source_file.parent}\n")
                f.write(f"üóÇÔ∏è –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {source_file.stat().st_size / (1024 * 1024):.1f} MB\n")
                f.write("\n")

                # –Ø–∑—ã–∫–∏
                f.write("üåê –Ø–ó–´–ö–ò\n")
                f.write("-" * 40 + "\n")
                f.write(f"üì• –ò—Å—Ö–æ–¥–Ω—ã–π —è–∑—ã–∫: {src_lang}\n")
                f.write(f"üì§ –¶–µ–ª–µ–≤–æ–π —è–∑—ã–∫: {tgt_lang}\n")
                detected = stats.get("languages_detected", {})
                if detected:
                    f.write(f"üîç –ê–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ –∏–∑ —Ñ–∞–π–ª–∞:\n")
                    f.write(f"   - Source: {detected.get('source', 'unknown')}\n")
                    f.write(f"   - Target: {detected.get('target', 'unknown')}\n")
                f.write("\n")

                # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
                f.write("üìä –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê\n")
                f.write("-" * 40 + "\n")
                f.write(f"üìã –í—Å–µ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –≤ SDLTM: {stats['total_in_sdltm']:,}\n")
                f.write(f"‚öôÔ∏è –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å–µ–≥–º–µ–Ω—Ç–æ–≤: {stats['processed']:,}\n")
                f.write(f"‚úÖ –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ –≤ TMX: {stats['exported']:,}\n")
                f.write(f"‚è±Ô∏è –í—Ä–µ–º—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏: {stats['conversion_time']:.2f} —Å–µ–∫—É–Ω–¥\n")
                f.write(f"üß† –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ –ø–∞–º—è—Ç–∏: {stats['memory_used_mb']:.1f} MB\n")
                f.write("\n")

                # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ–ø—É—Å–∫–æ–≤
                f.write("‚ö†Ô∏è –ü–†–û–ü–£–©–ï–ù–ù–´–ï –°–ï–ì–ú–ï–ù–¢–´\n")
                f.write("-" * 40 + "\n")
                f.write(f"üî∏ –ü—É—Å—Ç—ã–µ —Å–µ–≥–º–µ–Ω—Ç—ã: {stats['skipped_empty']:,}\n")
                f.write(f"üî∏ –¢–æ–ª—å–∫–æ —Ç–µ–≥–∏ (–±–µ–∑ —Ç–µ–∫—Å—Ç–∞): {stats['skipped_tags_only']:,}\n")
                f.write(f"üî∏ –î—É–±–ª–∏–∫–∞—Ç—ã: {stats['skipped_duplicates']:,}\n")
                f.write(f"üî∏ –û—à–∏–±–∫–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞: {stats['skipped_errors']:,}\n")

                total_skipped = (stats['skipped_empty'] + stats['skipped_tags_only'] +
                                 stats['skipped_duplicates'] + stats['skipped_errors'])
                f.write(f"üìä –ò—Ç–æ–≥–æ –ø—Ä–æ–ø—É—â–µ–Ω–æ: {total_skipped:,}\n")
                f.write(f"üìà –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å: {(stats['exported'] / stats['total_in_sdltm'] * 100):.1f}%\n")
                f.write("\n")

                # –°–æ–∑–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
                f.write("üì§ –°–û–ó–î–ê–ù–ù–´–ï –§–ê–ô–õ–´\n")
                f.write("-" * 40 + "\n")
                for output_file in output_files:
                    file_size = output_file.stat().st_size / (1024 * 1024) if output_file.exists() else 0
                    f.write(f"üìÑ {output_file.name} ({file_size:.1f} MB)\n")
                f.write(f"üìÑ {log_path.name} (—ç—Ç–æ—Ç –ª–æ–≥)\n")
                f.write("\n")

                # –î–ï–¢–ê–õ–¨–ù–´–ï –ü–†–ò–ú–ï–†–´ –ü–†–û–ü–£–©–ï–ù–ù–´–• –°–ï–ì–ú–ï–ù–¢–û–í
                skipped_details = detailed_stats["skipped_details"]

                if skipped_details["empty"]:
                    f.write("üîç –ü–†–ò–ú–ï–†–´ –ü–£–°–¢–´–• –°–ï–ì–ú–ï–ù–¢–û–í\n")
                    f.write("-" * 40 + "\n")
                    for i, (src, tgt) in enumerate(skipped_details["empty"][:5], 1):
                        f.write(f"  {i}. Source: '{src}'\n")
                        f.write(f"     Target: '{tgt}'\n")
                        f.write("\n")

                if skipped_details["tags_only"]:
                    f.write("üè∑Ô∏è –ü–†–ò–ú–ï–†–´ –°–ï–ì–ú–ï–ù–¢–û–í –¢–û–õ–¨–ö–û –° –¢–ï–ì–ê–ú–ò\n")
                    f.write("-" * 40 + "\n")
                    for i, (src, tgt) in enumerate(skipped_details["tags_only"][:5], 1):
                        f.write(f"  {i}. Source: '{src}'\n")
                        f.write(f"     Target: '{tgt}'\n")
                        f.write("\n")

                if skipped_details["duplicates"]:
                    f.write("üîÑ –ü–†–ò–ú–ï–†–´ –î–£–ë–õ–ò–ö–ê–¢–û–í\n")
                    f.write("-" * 40 + "\n")
                    for i, (src, tgt) in enumerate(skipped_details["duplicates"][:5], 1):
                        f.write(f"  {i}. Source: '{src}'\n")
                        f.write(f"     Target: '{tgt}'\n")
                        f.write("\n")

                if skipped_details["errors"]:
                    f.write("‚ùå –ü–†–ò–ú–ï–†–´ –û–®–ò–ë–û–ö –ü–ê–†–°–ò–ù–ì–ê\n")
                    f.write("-" * 40 + "\n")
                    for i, (src, tgt) in enumerate(skipped_details["errors"][:5], 1):
                        f.write(f"  {i}. Source: '{src}'\n")
                        f.write(f"     Target: '{tgt}'\n")
                        f.write("\n")

                # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
                f.write("üí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò\n")
                f.write("-" * 40 + "\n")

                if stats['skipped_empty'] > 0:
                    f.write(f"‚Ä¢ –ù–∞–π–¥–µ–Ω–æ {stats['skipped_empty']:,} –ø—É—Å—Ç—ã—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤. –≠—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ –¥–ª—è SDLTM —Ñ–∞–π–ª–æ–≤.\n")

                if stats['skipped_duplicates'] > stats['exported'] * 0.1:
                    f.write(f"‚Ä¢ –ú–Ω–æ–≥–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ ({stats['skipped_duplicates']:,}). –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –æ—á–∏—Å—Ç–∫—É –∏—Å—Ö–æ–¥–Ω–æ–π TM.\n")

                if stats['skipped_tags_only'] > 0:
                    f.write(
                        f"‚Ä¢ –ù–∞–π–¥–µ–Ω–æ {stats['skipped_tags_only']:,} —Å–µ–≥–º–µ–Ω—Ç–æ–≤ —Ç–æ–ª—å–∫–æ —Å —Ç–µ–≥–∞–º–∏. –≠—Ç–æ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Å–µ–≥–º–µ–Ω—Ç—ã.\n")

                efficiency = (stats['exported'] / stats['total_in_sdltm'] * 100)
                if efficiency > 80:
                    f.write("‚Ä¢ ‚úÖ –û—Ç–ª–∏—á–Ω–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏!\n")
                elif efficiency > 60:
                    f.write("‚Ä¢ ‚ö†Ô∏è –£–º–µ—Ä–µ–Ω–Ω–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å. –í–æ–∑–º–æ–∂–Ω–æ, –º–Ω–æ–≥–æ —Å–ª—É–∂–µ–±–Ω—ã—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤.\n")
                else:
                    f.write("‚Ä¢ ‚ùå –ù–∏–∑–∫–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–∞—á–µ—Å—Ç–≤–æ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞.\n")

                f.write("\n")
                f.write("=" * 80 + "\n")
                f.write("üîß –°–æ–∑–¥–∞–Ω–æ Converter Pro v2.0 - Professional TM/TB/TMX Converter\n")
                f.write("=" * 80 + "\n")

        except Exception as e:
            logger.error(f"Error writing conversion log: {e}")

    def _get_memory_usage(self) -> float:
        """
        –ü–æ–ª—É—á–∞–µ—Ç —Ç–µ–∫—É—â–µ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –≤ –ú–ë

        Returns:
            –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –≤ –ú–ë
        """
        try:
            import psutil
            process = psutil.Process()
            return process.memory_info().rss / 1024 / 1024
        except ImportError:
            return 0.0
        except Exception:
            return 0.0

    def _log_conversion_summary(self, filepath: Path, stats: Dict, src_lang: str, tgt_lang: str):
        """
        –õ–æ–≥–∏—Ä—É–µ—Ç –ø–æ–¥—Ä–æ–±–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏

        Args:
            filepath: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
            stats: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏
            src_lang: –ò—Å—Ö–æ–¥–Ω—ã–π —è–∑—ã–∫
            tgt_lang: –¶–µ–ª–µ–≤–æ–π —è–∑—ã–∫
        """
        logger.info("=" * 60)
        logger.info(f"CONVERSION SUMMARY: {filepath.name}")
        logger.info("=" * 60)
        logger.info(f"Languages: {src_lang} ‚Üí {tgt_lang}")
        logger.info(f"Total segments in SDLTM: {stats['total_in_sdltm']:,}")
        logger.info(f"Segments processed: {stats['processed']:,}")
        logger.info(f"Segments exported: {stats['exported']:,}")
        logger.info(f"Conversion time: {stats['conversion_time']:.2f} seconds")
        logger.info(f"Memory used: {stats['memory_used_mb']:.1f} MB")
        logger.info("")
        logger.info("SKIPPED SEGMENTS:")
        logger.info(f"  Empty segments: {stats['skipped_empty']:,}")
        logger.info(f"  Tag-only segments: {stats['skipped_tags_only']:,}")
        logger.info(f"  Duplicate segments: {stats['skipped_duplicates']:,}")
        logger.info(f"  Error segments: {stats['skipped_errors']:,}")
        logger.info("")
        logger.info("DETECTED LANGUAGES:")
        for lang_type, lang_code in stats['languages_detected'].items():
            logger.info(f"  {lang_type.capitalize()}: {lang_code}")
        logger.info("=" * 60)

    def get_supported_formats(self) -> Set[str]:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã —ç–∫—Å–ø–æ—Ä—Ç–∞

        Returns:
            –ú–Ω–æ–∂–µ—Å—Ç–≤–æ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤
        """
        return self.supported_exports.copy()

    def estimate_conversion_time(self, filepath: Path) -> float:
        """
        –û—Ü–µ–Ω–∏–≤–∞–µ—Ç –≤—Ä–µ–º—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞–∑–º–µ—Ä–∞ —Ñ–∞–π–ª–∞

        Args:
            filepath: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É

        Returns:
            –ü—Ä–∏–º–µ—Ä–Ω–æ–µ –≤—Ä–µ–º—è –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
        """
        try:
            file_size_mb = filepath.stat().st_size / (1024 * 1024)
            # –ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞: 1 –ú–ë = 5 —Å–µ–∫—É–Ω–¥
            return file_size_mb * 5
        except Exception:
            return 0.0

    def get_file_info(self, filepath: Path) -> Dict[str, any]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –ø–æ–¥—Ä–æ–±–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ SDLTM —Ñ–∞–π–ª–µ

        Args:
            filepath: –ü—É—Ç—å –∫ SDLTM —Ñ–∞–π–ª—É

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Ñ–∞–π–ª–µ
        """
        info = {
            "file_size_mb": 0,
            "total_segments": 0,
            "source_language": "unknown",
            "target_language": "unknown",
            "creation_date": None,
            "database_version": "unknown"
        }

        try:
            # –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
            info["file_size_mb"] = filepath.stat().st_size / (1024 * 1024)

            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
            with sqlite3.connect(str(filepath)) as conn:
                cursor = conn.cursor()

                # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–µ–≥–º–µ–Ω—Ç–æ–≤
                cursor.execute("SELECT COUNT(*) FROM translation_units")
                info["total_segments"] = cursor.fetchone()[0]

                # –ü—ã—Ç–∞–µ–º—Å—è –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —è–∑—ã–∫–∏ –∏–∑ –ø–µ—Ä–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π
                detected_langs = self._detect_languages(filepath)
                info["source_language"] = detected_langs.get("source", "unknown")
                info["target_language"] = detected_langs.get("target", "unknown")

                # –í–µ—Ä—Å–∏—è SQLite
                cursor.execute("SELECT sqlite_version()")
                info["database_version"] = cursor.fetchone()[0]

        except Exception as e:
            logger.warning(f"Could not get file info for {filepath}: {e}")

        return info

    def cleanup_temp_files(self, filepath: Path):
        """
        –û—á–∏—â–∞–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã SQLite

        Args:
            filepath: –ü—É—Ç—å –∫ –æ—Å–Ω–æ–≤–Ω–æ–º—É SDLTM —Ñ–∞–π–ª—É
        """
        temp_files_cleaned = 0

        for suffix in ['-wal', '-shm', '-journal']:
            temp_file = Path(str(filepath) + suffix)
            if temp_file.exists():
                try:
                    temp_file.unlink()
                    temp_files_cleaned += 1
                    logger.debug(f"Cleaned up SQLite temp file: {temp_file}")
                except Exception as e:
                    logger.warning(f"Could not clean temp file {temp_file}: {e}")

        if temp_files_cleaned > 0:
            logger.info(f"Cleaned up {temp_files_cleaned} SQLite temporary files")

    def validate_output_files(self, output_files: List[Path]) -> Dict[str, bool]:
        """
        –í–∞–ª–∏–¥–∏—Ä—É–µ—Ç —Å–æ–∑–¥–∞–Ω–Ω—ã–µ –≤—ã—Ö–æ–¥–Ω—ã–µ —Ñ–∞–π–ª—ã

        Args:
            output_files: –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ –≤—ã—Ö–æ–¥–Ω—ã–º —Ñ–∞–π–ª–∞–º

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ñ–∞–π–ª–∞
        """
        validation_results = {}

        for file_path in output_files:
            try:
                if not file_path.exists():
                    validation_results[str(file_path)] = False
                    continue

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
                if file_path.stat().st_size == 0:
                    validation_results[str(file_path)] = False
                    continue

                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–æ —Ç–∏–ø—É —Ñ–∞–π–ª–∞
                if file_path.suffix.lower() == '.tmx':
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ TMX —Ñ–∞–π–ª –≤–∞–ª–∏–¥–µ–Ω
                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            content = f.read(1000)  # –ß–∏—Ç–∞–µ–º –ø–µ—Ä–≤—ã–µ 1000 —Å–∏–º–≤–æ–ª–æ–≤
                            if '<tmx' in content and '</tmx>' in content:
                                validation_results[str(file_path)] = True
                            else:
                                validation_results[str(file_path)] = False
                    except Exception:
                        validation_results[str(file_path)] = False

                elif file_path.suffix.lower() in ['.xlsx', '.xls']:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º Excel —Ñ–∞–π–ª
                    try:
                        import openpyxl
                        wb = openpyxl.load_workbook(str(file_path), read_only=True)
                        wb.close()
                        validation_results[str(file_path)] = True
                    except Exception:
                        validation_results[str(file_path)] = False

                elif file_path.suffix.lower() == '.json':
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º JSON —Ñ–∞–π–ª
                    try:
                        import json
                        with open(file_path, 'r', encoding='utf-8') as f:
                            json.load(f)
                        validation_results[str(file_path)] = True
                    except Exception:
                        validation_results[str(file_path)] = False

                else:
                    # –î–ª—è –¥—Ä—É–≥–∏—Ö —Ç–∏–ø–æ–≤ —Ñ–∞–π–ª–æ–≤ –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –∏ —Ä–∞–∑–º–µ—Ä
                    validation_results[str(file_path)] = True

            except Exception as e:
                logger.warning(f"Error validating {file_path}: {e}")
                validation_results[str(file_path)] = False

        return validation_results

    def get_conversion_statistics(self, filepath: Path) -> Dict[str, any]:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–ª—è —Ñ–∞–π–ª–∞ (–±–µ–∑ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏)

        Args:
            filepath: –ü—É—Ç—å –∫ SDLTM —Ñ–∞–π–ª—É

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
        """
        stats = {
            "total_segments": 0,
            "estimated_valid_segments": 0,
            "estimated_empty_segments": 0,
            "estimated_conversion_time": 0,
            "file_size_mb": 0,
            "languages": {"source": "unknown", "target": "unknown"}
        }

        try:
            # –ë–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
            stats["file_size_mb"] = filepath.stat().st_size / (1024 * 1024)
            stats["estimated_conversion_time"] = self.estimate_conversion_time(filepath)

            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ –ë–î
            with sqlite3.connect(str(filepath)) as conn:
                cursor = conn.cursor()

                # –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
                cursor.execute("SELECT COUNT(*) FROM translation_units")
                stats["total_segments"] = cursor.fetchone()[0]

                # –Ø–∑—ã–∫–∏
                stats["languages"] = self._detect_languages(filepath)

                # –ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –≤–∞–ª–∏–¥–Ω—ã—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤ (–∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤—ã–±–æ—Ä–∫—É)
                cursor.execute("SELECT source_segment, target_segment FROM translation_units LIMIT 100")
                sample_valid = 0
                sample_empty = 0

                for src_xml, tgt_xml in cursor.fetchall():
                    try:
                        src_text, _ = self._parse_segment_xml(src_xml)
                        tgt_text, _ = self._parse_segment_xml(tgt_xml)

                        if src_text.strip() and tgt_text.strip():
                            if not (self._is_tags_only(src_xml) or self._is_tags_only(tgt_xml)):
                                sample_valid += 1
                            else:
                                sample_empty += 1
                        else:
                            sample_empty += 1
                    except Exception:
                        sample_empty += 1

                # –≠–∫—Å—Ç—Ä–∞–ø–æ–ª–∏—Ä—É–µ–º –Ω–∞ –≤–µ—Å—å —Ñ–∞–π–ª
                if sample_valid + sample_empty > 0:
                    valid_ratio = sample_valid / (sample_valid + sample_empty)
                    stats["estimated_valid_segments"] = int(stats["total_segments"] * valid_ratio)
                    stats["estimated_empty_segments"] = stats["total_segments"] - stats["estimated_valid_segments"]

        except Exception as e:
            logger.warning(f"Error getting conversion statistics for {filepath}: {e}")

        return stats